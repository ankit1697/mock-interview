{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "hlX-17z7Rzfi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eerd9c-JNxwW",
        "outputId": "551b5987-d05c-4ef8-aa21-a36868da6be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: google-cloud-speech in /usr/local/lib/python3.11/dist-packages (2.32.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (5.29.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2025.1.31)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.28)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.52)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.72.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (0.3.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain_openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain_openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain_openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Error reading PDF: [Errno 2] No such file or directory: 'resume.pdf'\n",
            "Error reading file jd.txt: [Errno 2] No such file or directory: 'jd.txt'\n",
            "Error reading PDF: [Errno 2] No such file or directory: 'resume.pdf'\n",
            "Resume parsed in 6.95 seconds\n",
            "Job description parsed in 1.27 seconds\n",
            "Initializing RAG with knowledge base...\n",
            "Error initializing RAG: Error loading data_science.txt\n",
            "Resume-job alignment analyzed in 9.4 seconds\n",
            "\n",
            "===== RESUME-JOB ALIGNMENT ANALYSIS =====\n",
            "Overall Match Score: 50/100\n",
            "\n",
            "Strengths:\n",
            "- Strong technical skills in JavaScript, Python, and Java\n",
            "- Experience with modern frameworks and tools like React, Node.js, Docker, and Jenkins\n",
            "- Proven leadership skills and experience leading a development team\n",
            "\n",
            "Areas of Concern:\n",
            "- No specific job description provided; unable to fully match against job requirements\n",
            "- Potential misalignment due to lack of clear job responsibilities and preferred skills\n",
            "\n",
            "Suggested Interview Focus Areas:\n",
            "- Clarification of specific job responsibilities and requirements\n",
            "- In-depth technical assessment, especially involving cloud architecture and microservices\n",
            "- Discussion of leadership experience and team management skills\n",
            "\n",
            "\n",
            "Generated 11 tailored questions in 4.79 seconds\n",
            "\n",
            "===== INTERVIEW STARTED =====\n",
            "\n",
            "Interviewer: You have experience with both JavaScript and Node.js. Can you explain how you would manage asynchronous operations in Node.js and describe a situation where you leveraged this capability effectively in your projects?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): good in cloud platforms\n",
            "\n",
            "Answer Evaluation:\n",
            "**Technical Accuracy (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The candidate's response does not demonstrate any understanding of asynchronous operations in Node.js or any related technical concepts.  \n",
            "Improvement Suggestion: Provide an explanation of how asynchronous operations are managed in Node.js, such as using callbacks, promises, or async/await.\n",
            "\n",
            "**Relevance (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The candidate's answer regarding cloud platforms is not relevant to the question about asynchronous operations in Node.js.  \n",
            "Improvement Suggestion: Focus on responding specifically to the question about Node.js and asynchronous processes.\n",
            "\n",
            "**Depth (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: There is no exploration of the topic, as the response does not touch upon asynchronous operations or provide any context.  \n",
            "Improvement Suggestion: Delve into specific strategies such as event loops, callbacks, or promises, and explain their role and advantages in managing asynchronous tasks.\n",
            "\n",
            "**Communication (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The answer lacks clarity and structure since it does not address the question.  \n",
            "Improvement Suggestion: Structure the response to first explain the technical concepts and then provide a real-world example, ensuring each part clearly relates to the question asked.\n",
            "\n",
            "**Practical Application (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The answer does not connect to any practical experience or real-world application.  \n",
            "Improvement Suggestion: Include an example of a project where you effectively used asynchronous operations in Node.js, describing the situation and the outcome.\n",
            "\n",
            "**Overall Score (0-100):** 0  \n",
            "\n",
            "**Summary Feedback:**  \n",
            "The candidate's answer did not address the question posed and failed to demonstrate knowledge of managing asynchronous operations in Node.js. To improve, the candidate should discuss relevant technical concepts such as callbacks, promises, and async/await. Additionally, providing a detailed example of how these concepts were applied in a past project would enhance the response's relevance, depth, and practical application.\n",
            "Took 6.81 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "**Technical Accuracy (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The candidate's response does not demonstrate any understanding of asynchronous operations in Node.js or any related technical concepts.  \n",
            "Improvement Suggestion: Provide an explanation of how asynchronous operations are managed in Node.js, such as using callbacks, promises, or async/await.\n",
            "\n",
            "**Relevance (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The candidate's answer regarding cloud platforms is not relevant to the question about asynchronous operations in Node.js.  \n",
            "Improvement Suggestion: Focus on responding specifically to the question about Node.js and asynchronous processes.\n",
            "\n",
            "**Depth (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: There is no exploration of the topic, as the response does not touch upon asynchronous operations or provide any context.  \n",
            "Improvement Suggestion: Delve into specific strategies such as event loops, callbacks, or promises, and explain their role and advantages in managing asynchronous tasks.\n",
            "\n",
            "**Communication (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The answer lacks clarity and structure since it does not address the question.  \n",
            "Improvement Suggestion: Structure the response to first explain the technical concepts and then provide a real-world example, ensuring each part clearly relates to the question asked.\n",
            "\n",
            "**Practical Application (0-10):**  \n",
            "Score: 0  \n",
            "Feedback: The answer does not connect to any practical experience or real-world application.  \n",
            "Improvement Suggestion: Include an example of a project where you effectively used asynchronous operations in Node.js, describing the situation and the outcome.\n",
            "\n",
            "**Overall Score (0-100):** 0  \n",
            "\n",
            "**Summary Feedback:**  \n",
            "The candidate's answer did not address the question posed and failed to demonstrate knowledge of managing asynchronous operations in Node.js. To improve, the candidate should discuss relevant technical concepts such as callbacks, promises, and async/await. Additionally, providing a detailed example of how these concepts were applied in a past project would enhance the response's relevance, depth, and practical application.\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: In e-commerce web development, ensuring a smooth checkout process is crucial. If a client reported intermittent slowdowns during peak hours at checkout, how would you diagnose and address the issue to improve performance?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): have effiecient management\n",
            "\n",
            "Answer Evaluation:\n",
            "**Technical Accuracy: 1**  \n",
            "- Feedback: The candidate's response is too vague and does not demonstrate any understanding of the technical concepts required to address the problem of slowdowns during checkout.  \n",
            "- Improvement: Suggest exploring the potential causes of slowdowns, such as server load, network latency, or database performance, and referencing specific technologies or strategies used in diagnosing and fixing such issues.\n",
            "\n",
            "**Relevance: 1**  \n",
            "- Feedback: The response fails to directly address the question of diagnosing and addressing performance issues in an e-commerce checkout process.  \n",
            "- Improvement: Focus on relevant strategies like analyzing server logs, monitoring real-time performance metrics, or identifying bottlenecks in the backend infrastructure.\n",
            "\n",
            "**Depth: 0**  \n",
            "- Feedback: The answer lacks any depth or exploration of the topic.  \n",
            "- Improvement: Provide detailed steps or methodologies for troubleshooting the checkout process, such as performing load testing or using performance monitoring tools to diagnose the problem.\n",
            "\n",
            "**Communication: 1**  \n",
            "- Feedback: The answer lacks clarity and structure, making it difficult to understand the candidates viewpoint.  \n",
            "- Improvement: Elaborate on the response with complete sentences and structured thoughts that convey a clear diagnostic approach.\n",
            "\n",
            "**Practical Application: 0**  \n",
            "- Feedback: There is no connection to practical solutions or actions that could be taken in a real-world scenario.  \n",
            "- Improvement: Incorporate practical examples, such as implementing caching or optimizing database queries, to show how the checkout process could be improved in a live environment.\n",
            "\n",
            "**Overall Score: 3/100**  \n",
            "- Summary Feedback: The candidate's response is too brief and lacks any meaningful content related to diagnosing and addressing checkout slowdowns in an e-commerce environment. There is no mention of specific strategies or technologies, indicating a lack of understanding and experience in handling such issues. It is crucial to provide a more comprehensive and technically sound answer, touching upon potential causes, diagnostic tools, and preventative measures.\n",
            "Took 6.33 seconds\n",
            "\n",
            "Using OpenAI for question generation\n",
            "\n",
            "Dynamic Follow-Up Question: You mentioned efficient management; can you elaborate on how you prioritize tasks in a data science project to ensure timely delivery and quality outcomes?\n",
            "Took 1.25 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "**Technical Accuracy: 1**  \n",
            "- Feedback: The candidate's response is too vague and does not demonstrate any understanding of the technical concepts required to address the problem of slowdowns during checkout.  \n",
            "- Improvement: Suggest exploring the potential causes of slowdowns, such as server load, network latency, or database performance, and referencing specific technologies or strategies used in diagnosing and fixing such issues.\n",
            "\n",
            "**Relevance: 1**  \n",
            "- Feedback: The response fails to directly address the question of diagnosing and addressing performance issues in an e-commerce checkout process.  \n",
            "- Improvement: Focus on relevant strategies like analyzing server logs, monitoring real-time performance metrics, or identifying bottlenecks in the backend infrastructure.\n",
            "\n",
            "**Depth: 0**  \n",
            "- Feedback: The answer lacks any depth or exploration of the topic.  \n",
            "- Improvement: Provide detailed steps or methodologies for troubleshooting the checkout process, such as performing load testing or using performance monitoring tools to diagnose the problem.\n",
            "\n",
            "**Communication: 1**  \n",
            "- Feedback: The answer lacks clarity and structure, making it difficult to understand the candidates viewpoint.  \n",
            "- Improvement: Elaborate on the response with complete sentences and structured thoughts that convey a clear diagnostic approach.\n",
            "\n",
            "**Practical Application: 0**  \n",
            "- Feedback: There is no connection to practical solutions or actions that could be taken in a real-world scenario.  \n",
            "- Improvement: Incorporate practical examples, such as implementing caching or optimizing database queries, to show how the checkout process could be improved in a live environment.\n",
            "\n",
            "**Overall Score: 3/100**  \n",
            "- Summary Feedback: The candidate's response is too brief and lacks any meaningful content related to diagnosing and addressing checkout slowdowns in an e-commerce environment. There is no mention of specific strategies or technologies, indicating a lack of understanding and experience in handling such issues. It is crucial to provide a more comprehensive and technically sound answer, touching upon potential causes, diagnostic tools, and preventative measures.\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: You mentioned efficient management; can you elaborate on how you prioritize tasks in a data science project to ensure timely delivery and quality outcomes?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): good communication\n",
            "\n",
            "Answer Evaluation:\n",
            "1. Technical Accuracy (0-10): 1\n",
            "   - Feedback: The candidate's response does not demonstrate any understanding of technical concepts related to task prioritization in data science.\n",
            "   - Improvement Suggestion: Provide specific techniques or frameworks used in project management, like Agile, Kanban, or priority matrices, to demonstrate understanding.\n",
            "\n",
            "2. Relevance (0-10): 1\n",
            "   - Feedback: The answer does not directly address the question about prioritizing tasks.\n",
            "   - Improvement Suggestion: Directly address how tasks are prioritized in data science by mentioning aspects like project goals, deadlines, resource allocation, and impact assessment.\n",
            "\n",
            "3. Depth (0-10): 1\n",
            "   - Feedback: The answer lacks depth as it simply mentions \"good communication\" without elaboration or connection to task management.\n",
            "   - Improvement Suggestion: Expand on methods used for prioritizing tasks, such as balancing time-sensitive tasks with high-impact projects and setting milestones.\n",
            "\n",
            "4. Communication (0-10): 2\n",
            "   - Feedback: The response is concise but lacks clarity and structure, providing no substantial information.\n",
            "   - Improvement Suggestion: Aim to provide a structured answer beginning with overall strategy, then detail specific steps or tools used.\n",
            "\n",
            "5. Practical Application (0-10): 1\n",
            "   - Feedback: The response does not connect to real-world applications in data science projects.\n",
            "   - Improvement Suggestion: Include examples or scenarios where specific prioritization techniques led to successful project outcomes.\n",
            "\n",
            "Overall Score (0-100): 6\n",
            "\n",
            "Summary Feedback: The candidate's response is inadequate as it fails to address the question about task prioritization in data science projects. The brief mention of \"good communication\" does not provide any insight into effective management or prioritization strategies. To improve, the candidate should provide specific methodologies, frameworks, or real-world examples that illustrate how tasks can be efficiently managed to ensure timely delivery and quality outcomes in data science projects.\n",
            "Took 7.21 seconds\n",
            "\n",
            "Using OpenAI for question generation\n",
            "\n",
            "Dynamic Follow-Up Question: Can you share an example of how you effectively communicated complex data insights to a non-technical audience, and what strategies you used to ensure understanding?\n",
            "Took 0.92 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "1. Technical Accuracy (0-10): 1\n",
            "   - Feedback: The candidate's response does not demonstrate any understanding of technical concepts related to task prioritization in data science.\n",
            "   - Improvement Suggestion: Provide specific techniques or frameworks used in project management, like Agile, Kanban, or priority matrices, to demonstrate understanding.\n",
            "\n",
            "2. Relevance (0-10): 1\n",
            "   - Feedback: The answer does not directly address the question about prioritizing tasks.\n",
            "   - Improvement Suggestion: Directly address how tasks are prioritized in data science by mentioning aspects like project goals, deadlines, resource allocation, and impact assessment.\n",
            "\n",
            "3. Depth (0-10): 1\n",
            "   - Feedback: The answer lacks depth as it simply mentions \"good communication\" without elaboration or connection to task management.\n",
            "   - Improvement Suggestion: Expand on methods used for prioritizing tasks, such as balancing time-sensitive tasks with high-impact projects and setting milestones.\n",
            "\n",
            "4. Communication (0-10): 2\n",
            "   - Feedback: The response is concise but lacks clarity and structure, providing no substantial information.\n",
            "   - Improvement Suggestion: Aim to provide a structured answer beginning with overall strategy, then detail specific steps or tools used.\n",
            "\n",
            "5. Practical Application (0-10): 1\n",
            "   - Feedback: The response does not connect to real-world applications in data science projects.\n",
            "   - Improvement Suggestion: Include examples or scenarios where specific prioritization techniques led to successful project outcomes.\n",
            "\n",
            "Overall Score (0-100): 6\n",
            "\n",
            "Summary Feedback: The candidate's response is inadequate as it fails to address the question about task prioritization in data science projects. The brief mention of \"good communication\" does not provide any insight into effective management or prioritization strategies. To improve, the candidate should provide specific methodologies, frameworks, or real-world examples that illustrate how tasks can be efficiently managed to ensure timely delivery and quality outcomes in data science projects.\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: Can you share an example of how you effectively communicated complex data insights to a non-technical audience, and what strategies you used to ensure understanding?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): have better visualizations to communicate easily\n",
            "\n",
            "Answer Evaluation:\n",
            "1. Technical Accuracy: 2/10  \n",
            "   - Feedback: The candidate's answer lacks technical detail and doesn't demonstrate an understanding of how to create or utilize visualizations effectively.  \n",
            "   - Improvement Suggestion: Provide specific examples of visualization tools or techniques used to effectively communicate data insights.\n",
            "\n",
            "2. Relevance: 3/10  \n",
            "   - Feedback: The answer is somewhat relevant as it mentions visualizations, but it doesn't directly address the strategies to ensure understanding by a non-technical audience.  \n",
            "   - Improvement Suggestion: Discuss specific strategies, such as simplifying technical jargon or using storytelling alongside visualizations.\n",
            "\n",
            "3. Depth: 1/10  \n",
            "   - Feedback: The answer is very brief and does not explore the topic beyond mentioning \"better visualizations.\"  \n",
            "   - Improvement Suggestion: Elaborate on various methods and tools for visualization and contextual adaptation for different audiences.\n",
            "\n",
            "4. Communication: 2/10  \n",
            "   - Feedback: The answer is not well-structured and lacks clarity. It does not cover the components necessary to convey the message effectively to the intended audience.  \n",
            "   - Improvement Suggestion: Structure the answer with an introduction, explanation of the approach, and conclusion to enhance clarity.\n",
            "\n",
            "5. Practical Application: 1/10  \n",
            "   - Feedback: The answer does not convincingly show how the suggested approach could be applied in a real-world scenario.  \n",
            "   - Improvement Suggestion: Provide a specific example or case study of a time when visualizations effectively communicated complex data to a non-technical audience.\n",
            "\n",
            "Overall Score: 18/100\n",
            "\n",
            "Summary Feedback: The candidate's response lacks detail and depth and does not adequately address the question. It briefly mentions the role of visualizations but fails to describe the steps or strategies involved in making complex data accessible to non-technical audiences. To improve, the candidate should incorporate specific examples, structured communication, and detailed strategies for effective data presentation in real-world scenarios.\n",
            "Took 7.44 seconds\n",
            "\n",
            "Using OpenAI for question generation\n",
            "\n",
            "Dynamic Follow-Up Question: How do you determine which visualization type best communicates your data insights, and can you provide an example where this choice significantly impacted stakeholder understanding?\n",
            "Took 1.3 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "1. Technical Accuracy: 2/10  \n",
            "   - Feedback: The candidate's answer lacks technical detail and doesn't demonstrate an understanding of how to create or utilize visualizations effectively.  \n",
            "   - Improvement Suggestion: Provide specific examples of visualization tools or techniques used to effectively communicate data insights.\n",
            "\n",
            "2. Relevance: 3/10  \n",
            "   - Feedback: The answer is somewhat relevant as it mentions visualizations, but it doesn't directly address the strategies to ensure understanding by a non-technical audience.  \n",
            "   - Improvement Suggestion: Discuss specific strategies, such as simplifying technical jargon or using storytelling alongside visualizations.\n",
            "\n",
            "3. Depth: 1/10  \n",
            "   - Feedback: The answer is very brief and does not explore the topic beyond mentioning \"better visualizations.\"  \n",
            "   - Improvement Suggestion: Elaborate on various methods and tools for visualization and contextual adaptation for different audiences.\n",
            "\n",
            "4. Communication: 2/10  \n",
            "   - Feedback: The answer is not well-structured and lacks clarity. It does not cover the components necessary to convey the message effectively to the intended audience.  \n",
            "   - Improvement Suggestion: Structure the answer with an introduction, explanation of the approach, and conclusion to enhance clarity.\n",
            "\n",
            "5. Practical Application: 1/10  \n",
            "   - Feedback: The answer does not convincingly show how the suggested approach could be applied in a real-world scenario.  \n",
            "   - Improvement Suggestion: Provide a specific example or case study of a time when visualizations effectively communicated complex data to a non-technical audience.\n",
            "\n",
            "Overall Score: 18/100\n",
            "\n",
            "Summary Feedback: The candidate's response lacks detail and depth and does not adequately address the question. It briefly mentions the role of visualizations but fails to describe the steps or strategies involved in making complex data accessible to non-technical audiences. To improve, the candidate should incorporate specific examples, structured communication, and detailed strategies for effective data presentation in real-world scenarios.\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: How do you determine which visualization type best communicates your data insights, and can you provide an example where this choice significantly impacted stakeholder understanding?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): by understanding what the data is showing and then analysing it\n",
            "\n",
            "Answer Evaluation:\n",
            "1. Technical Accuracy (2/10):\n",
            "   - Feedback: The candidate's answer does not demonstrate a clear understanding of how to determine appropriate visualization types. The explanation lacks key technical concepts like identifying the nature of the data, the type of comparison needed, or audience requirements.\n",
            "   - Improvement Suggestion: The candidate should familiarize themselves with visualization principles, such as knowing when to use bar charts, line graphs, scatter plots, etc., based on data characteristics and stakeholder needs.\n",
            "\n",
            "2. Relevance (3/10):\n",
            "   - Feedback: The candidate only partially addresses the question by mentioning data understanding but fails to connect this to the choice of visualization types or stakeholder communication.\n",
            "   - Improvement Suggestion: The candidate should focus on detailing the process of mapping data insights to appropriate visualizations and consider stakeholder comprehension as an explicit factor.\n",
            "\n",
            "3. Depth (2/10):\n",
            "   - Feedback: The answer is very shallow and does not explore key factors such as data types, visualization goals, or the stakeholder's perspective.\n",
            "   - Improvement Suggestion: Providing examples of different data scenarios (time series, categorical data) and corresponding visualizations would add depth to the response.\n",
            "\n",
            "4. Communication (3/10):\n",
            "   - Feedback: The candidate's explanation is vague and lacks structure, making it difficult to follow their line of reasoning.\n",
            "   - Improvement Suggestion: Structuring the answer to first identify data visualization principles and then applying these to a specific example would greatly enhance clarity.\n",
            "\n",
            "5. Practical Application (1/10):\n",
            "   - Feedback: The candidate did not provide a practical example or demonstrate how visualization choice impacted stakeholder understanding.\n",
            "   - Improvement Suggestion: Integrate a case study or past experience where a particular visualization improved stakeholder decisions or communication.\n",
            "\n",
            "Overall Score (11/100):\n",
            "- Summary Feedback: The candidate's response is very limited in addressing the interview question. Essential components such as the types of data, selection criteria for visualizations, and an example are missing. The candidate should enhance their understanding of data visualization principles, learn to apply these in practical scenarios, and communicate effectively to illustrate their insights. Engaging with case studies or real-world examples could solidify their grasp of this concept.\n",
            "Took 8.97 seconds\n",
            "\n",
            "Using OpenAI for question generation\n",
            "\n",
            "Dynamic Follow-Up Question: How do you ensure your analysis accurately reflects the underlying data trends, and what techniques do you use to validate your findings?\n",
            "Took 0.82 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "1. Technical Accuracy (2/10):\n",
            "   - Feedback: The candidate's answer does not demonstrate a clear understanding of how to determine appropriate visualization types. The explanation lacks key technical concepts like identifying the nature of the data, the type of comparison needed, or audience requirements.\n",
            "   - Improvement Suggestion: The candidate should familiarize themselves with visualization principles, such as knowing when to use bar charts, line graphs, scatter plots, etc., based on data characteristics and stakeholder needs.\n",
            "\n",
            "2. Relevance (3/10):\n",
            "   - Feedback: The candidate only partially addresses the question by mentioning data understanding but fails to connect this to the choice of visualization types or stakeholder communication.\n",
            "   - Improvement Suggestion: The candidate should focus on detailing the process of mapping data insights to appropriate visualizations and consider stakeholder comprehension as an explicit factor.\n",
            "\n",
            "3. Depth (2/10):\n",
            "   - Feedback: The answer is very shallow and does not explore key factors such as data types, visualization goals, or the stakeholder's perspective.\n",
            "   - Improvement Suggestion: Providing examples of different data scenarios (time series, categorical data) and corresponding visualizations would add depth to the response.\n",
            "\n",
            "4. Communication (3/10):\n",
            "   - Feedback: The candidate's explanation is vague and lacks structure, making it difficult to follow their line of reasoning.\n",
            "   - Improvement Suggestion: Structuring the answer to first identify data visualization principles and then applying these to a specific example would greatly enhance clarity.\n",
            "\n",
            "5. Practical Application (1/10):\n",
            "   - Feedback: The candidate did not provide a practical example or demonstrate how visualization choice impacted stakeholder understanding.\n",
            "   - Improvement Suggestion: Integrate a case study or past experience where a particular visualization improved stakeholder decisions or communication.\n",
            "\n",
            "Overall Score (11/100):\n",
            "- Summary Feedback: The candidate's response is very limited in addressing the interview question. Essential components such as the types of data, selection criteria for visualizations, and an example are missing. The candidate should enhance their understanding of data visualization principles, learn to apply these in practical scenarios, and communicate effectively to illustrate their insights. Engaging with case studies or real-world examples could solidify their grasp of this concept.\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: How do you ensure your analysis accurately reflects the underlying data trends, and what techniques do you use to validate your findings?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): proper EDA\n",
            "\n",
            "Answer Evaluation:\n",
            "1. Technical Accuracy: 2\n",
            "   - The candidate's answer lacks specificity and fails to demonstrate understanding of the technical processes involved in ensuring accurate analysis or validating findings.\n",
            "   - Improvement suggestion: The candidate should mention specific techniques like statistical tests, cross-validation, or anomaly detection that are used for validating trends and accuracy.\n",
            "\n",
            "2. Relevance: 3\n",
            "   - While exploratory data analysis (EDA) is relevant to understanding data trends, the answer doesn't explain how EDA ensures accurate reflection of these trends or validation of findings.\n",
            "   - Improvement suggestion: The candidate should elaborate on how EDA contributes to understanding data trends and its limitations in ensuring accuracy.\n",
            "\n",
            "3. Depth: 1\n",
            "   - The answer is extremely superficial and does not explore any aspects of the question in detail.\n",
            "   - Improvement suggestion: The candidate should provide further insights into techniques used during EDA and include other validation methods such as hypothesis testing and model evaluation metrics.\n",
            "\n",
            "4. Communication: 2\n",
            "   - The answer is clear in its brevity but lacks the necessary detail to convey understanding or relevance to the question asked.\n",
            "   - Improvement suggestion: The candidate should aim to provide answers with more detailed explanations and examples to effectively communicate their approach.\n",
            "\n",
            "5. Practical Application: 1\n",
            "   - The answer does not connect techniques to real-world applications or examples of how data trends can be misleading.\n",
            "   - Improvement suggestion: The candidate could improve by discussing a practical example, such as dataset-specific challenges and how those were addressed to ensure accurate analysis.\n",
            "\n",
            "Overall Score: 18/100\n",
            "\n",
            "Summary Feedback: The candidate's answer indicates a minimal grasp of the technical, relevant, and practical elements necessary for ensuring analysis accuracy and validating findings. To improve, the candidate should explain specific techniques and provide tangible examples from real-world data science scenarios. They should also aim to deliver more structured and comprehensive responses to demonstrate their understanding and competence effectively.\n",
            "\n",
            "Took 7.66 seconds\n",
            "\n",
            "Using OpenAI for question generation\n",
            "\n",
            "Dynamic Follow-Up Question: You mentioned proper EDA; how do you decide which visualization techniques to use for different types of data during this process?\n",
            "Took 1.0 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "1. Technical Accuracy: 2\n",
            "   - The candidate's answer lacks specificity and fails to demonstrate understanding of the technical processes involved in ensuring accurate analysis or validating findings.\n",
            "   - Improvement suggestion: The candidate should mention specific techniques like statistical tests, cross-validation, or anomaly detection that are used for validating trends and accuracy.\n",
            "\n",
            "2. Relevance: 3\n",
            "   - While exploratory data analysis (EDA) is relevant to understanding data trends, the answer doesn't explain how EDA ensures accurate reflection of these trends or validation of findings.\n",
            "   - Improvement suggestion: The candidate should elaborate on how EDA contributes to understanding data trends and its limitations in ensuring accuracy.\n",
            "\n",
            "3. Depth: 1\n",
            "   - The answer is extremely superficial and does not explore any aspects of the question in detail.\n",
            "   - Improvement suggestion: The candidate should provide further insights into techniques used during EDA and include other validation methods such as hypothesis testing and model evaluation metrics.\n",
            "\n",
            "4. Communication: 2\n",
            "   - The answer is clear in its brevity but lacks the necessary detail to convey understanding or relevance to the question asked.\n",
            "   - Improvement suggestion: The candidate should aim to provide answers with more detailed explanations and examples to effectively communicate their approach.\n",
            "\n",
            "5. Practical Application: 1\n",
            "   - The answer does not connect techniques to real-world applications or examples of how data trends can be misleading.\n",
            "   - Improvement suggestion: The candidate could improve by discussing a practical example, such as dataset-specific challenges and how those were addressed to ensure accurate analysis.\n",
            "\n",
            "Overall Score: 18/100\n",
            "\n",
            "Summary Feedback: The candidate's answer indicates a minimal grasp of the technical, relevant, and practical elements necessary for ensuring analysis accuracy and validating findings. To improve, the candidate should explain specific techniques and provide tangible examples from real-world data science scenarios. They should also aim to deliver more structured and comprehensive responses to demonstrate their understanding and competence effectively.\n",
            "\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: You mentioned proper EDA; how do you decide which visualization techniques to use for different types of data during this process?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): proper understanding of data\n",
            "\n",
            "Answer Evaluation:\n",
            "Based on the evaluation criteria, here is an assessment of the candidate's answer:\n",
            "\n",
            "1. **Technical Accuracy (0-10):**  \n",
            "   - Score: 2  \n",
            "   - Feedback: The candidate's answer lacks specific technical details about visualization techniques or a systematic approach for choosing them. It only provides a vague notion of understanding data.  \n",
            "   - Improvement: The candidate should study common visualization techniques like histograms, scatter plots, and heatmaps, and understand their appropriate contexts based on data type and analysis goals.\n",
            "\n",
            "2. **Relevance (0-10):**  \n",
            "   - Score: 1  \n",
            "   - Feedback: The response does not directly address the question about choosing visualization techniques in EDA.  \n",
            "   - Improvement: The candidate should focus the response more directly on the criteria for selecting different visualization methods according to data types.\n",
            "\n",
            "3. **Depth (0-10):**  \n",
            "   - Score: 1  \n",
            "   - Feedback: The explanation lacks depth, missing both foundational and advanced details of EDA visualization decision-making.  \n",
            "   - Improvement: Expand the answer to include considerations like distribution, dimensionality, and variable relationships in choosing visualizations.\n",
            "\n",
            "4. **Communication (0-10):**  \n",
            "   - Score: 2  \n",
            "   - Feedback: The answer is extremely brief and lacks any structured points.  \n",
            "   - Improvement: Construct a clearer, more organized response that includes key points and examples.\n",
            "\n",
            "5. **Practical Application (0-10):**  \n",
            "   - Score: 1  \n",
            "   - Feedback: There is no evidence of practical application or how the principles might be applied in real-world data analysis tasks.  \n",
            "   - Improvement: Provide examples of how different visualization techniques can be applied to various datasets in practical contexts or projects.\n",
            "\n",
            "**Overall Score (0-100):** 14\n",
            "\n",
            "**Summary Feedback:** The candidate's response is significantly lacking in content and clarity. It fails to address the specific question and lacks the depth needed for a structured discussion on selecting visualization techniques for different data types. To improve, the candidate should learn and articulate specific visualization methods and their applications in the context of exploratory data analysis, demonstrating a connection to real-world problem-solving.\n",
            "Took 8.03 seconds\n",
            "\n",
            "Using OpenAI for question generation\n",
            "\n",
            "Dynamic Follow-Up Question: Can you elaborate on how you ensure data quality and integrity before analysis, and what specific techniques you use to handle missing or inconsistent data?\n",
            "Took 0.85 seconds\n",
            "\n",
            "\n",
            "----- Evaluation -----\n",
            "Based on the evaluation criteria, here is an assessment of the candidate's answer:\n",
            "\n",
            "1. **Technical Accuracy (0-10):**  \n",
            "   - Score: 2  \n",
            "   - Feedback: The candidate's answer lacks specific technical details about visualization techniques or a systematic approach for choosing them. It only provides a vague notion of understanding data.  \n",
            "   - Improvement: The candidate should study common visualization techniques like histograms, scatter plots, and heatmaps, and understand their appropriate contexts based on data type and analysis goals.\n",
            "\n",
            "2. **Relevance (0-10):**  \n",
            "   - Score: 1  \n",
            "   - Feedback: The response does not directly address the question about choosing visualization techniques in EDA.  \n",
            "   - Improvement: The candidate should focus the response more directly on the criteria for selecting different visualization methods according to data types.\n",
            "\n",
            "3. **Depth (0-10):**  \n",
            "   - Score: 1  \n",
            "   - Feedback: The explanation lacks depth, missing both foundational and advanced details of EDA visualization decision-making.  \n",
            "   - Improvement: Expand the answer to include considerations like distribution, dimensionality, and variable relationships in choosing visualizations.\n",
            "\n",
            "4. **Communication (0-10):**  \n",
            "   - Score: 2  \n",
            "   - Feedback: The answer is extremely brief and lacks any structured points.  \n",
            "   - Improvement: Construct a clearer, more organized response that includes key points and examples.\n",
            "\n",
            "5. **Practical Application (0-10):**  \n",
            "   - Score: 1  \n",
            "   - Feedback: There is no evidence of practical application or how the principles might be applied in real-world data analysis tasks.  \n",
            "   - Improvement: Provide examples of how different visualization techniques can be applied to various datasets in practical contexts or projects.\n",
            "\n",
            "**Overall Score (0-100):** 14\n",
            "\n",
            "**Summary Feedback:** The candidate's response is significantly lacking in content and clarity. It fails to address the specific question and lacks the depth needed for a structured discussion on selecting visualization techniques for different data types. To improve, the candidate should learn and articulate specific visualization methods and their applications in the context of exploratory data analysis, demonstrating a connection to real-world problem-solving.\n",
            "\n",
            "----- Expert Example Answer -----\n",
            "Expert answers require a retrieval QA system and personal profile.\n",
            "\n",
            "----- Next Question -----\n",
            "Interviewer: Can you elaborate on how you ensure data quality and integrity before analysis, and what specific techniques you use to handle missing or inconsistent data?\n",
            "\n",
            "Your Answer (or type 'exit' to end the interview): exit\n",
            "\n",
            "===== INTERVIEW COMPLETED =====\n",
            "\n",
            "\n",
            "Interview Summary:\n",
            "1. **Overall Assessment:**\n",
            "   The candidate's performance in the interview was highly unsatisfactory. Throughout the questions, the responses were vague and lacked technical detail, depth, and relevance. The candidate failed to directly address most questions, often veering off-topic with answers that provided little to no insight into the data science concepts being assessed. The understanding of core data science principles, methodologies, and tools appeared to be minimal, as illustrated by the inability to discuss fundamental concepts like asynchronous operations, visualization techniques, and data integrity. Furthermore, the candidate's communication was not structured, concise, or clear enough to effectively convey any significant understanding of the topics addressed.\n",
            "\n",
            "2. **Key Strengths:**\n",
            "   - None noted, as the responses did not exhibit any discernible strengths in terms of technical content or relevant experience in data science.\n",
            "\n",
            "3. **Areas for Improvement:**\n",
            "   - **Technical Understanding:** The candidate should focus on learning and understanding key data science concepts, methodologies, and tools. Deepening their knowledge in areas such as data management, visualization techniques, and analysis verification is crucial.\n",
            "   - **Relevance and Depth:** There needs to be a focus on providing comprehensive and directly relevant responses to the questions. Practicing interview questions and answers could help in gaining confidence and producing more substantive replies.\n",
            "   - **Communication Skills:** The candidate should work on organizing and structuring their responses to ensure they are clear and coherent, which would involve providing specific examples and elaborating on points made.\n",
            "\n",
            "4. **Technical Competency: 1/10:**\n",
            "   The candidate displayed minimal technical knowledge pertinent to the data science role. There were no substantial references to specific methodologies, tools, or examples that would demonstrate competence in data handling, analysis, or visualization.\n",
            "\n",
            "5. **Communication Skills: 1/10:**\n",
            "   The communication skills demonstrated were insufficient for a data science role. The responses were brief, lacked structure, and did not adequately communicate understanding or sufficient detail to inform the interviewer.\n",
            "\n",
            "6. **Recommendation: Do Not Recommend**\n",
            "   Based on the demonstrated lack of technical knowledge, inadequate communication skills, and the inability to provide relevant responses or examples, it is not recommended to proceed with this candidate for the data science role. Significant improvement and preparation would be necessary before a reconsideration for such a position.\n",
            "Took 7.74 seconds\n",
            "\n",
            "\n",
            "===== INTERVIEW SUMMARY =====\n",
            "\n",
            "1. **Overall Assessment:**\n",
            "   The candidate's performance in the interview was highly unsatisfactory. Throughout the questions, the responses were vague and lacked technical detail, depth, and relevance. The candidate failed to directly address most questions, often veering off-topic with answers that provided little to no insight into the data science concepts being assessed. The understanding of core data science principles, methodologies, and tools appeared to be minimal, as illustrated by the inability to discuss fundamental concepts like asynchronous operations, visualization techniques, and data integrity. Furthermore, the candidate's communication was not structured, concise, or clear enough to effectively convey any significant understanding of the topics addressed.\n",
            "\n",
            "2. **Key Strengths:**\n",
            "   - None noted, as the responses did not exhibit any discernible strengths in terms of technical content or relevant experience in data science.\n",
            "\n",
            "3. **Areas for Improvement:**\n",
            "   - **Technical Understanding:** The candidate should focus on learning and understanding key data science concepts, methodologies, and tools. Deepening their knowledge in areas such as data management, visualization techniques, and analysis verification is crucial.\n",
            "   - **Relevance and Depth:** There needs to be a focus on providing comprehensive and directly relevant responses to the questions. Practicing interview questions and answers could help in gaining confidence and producing more substantive replies.\n",
            "   - **Communication Skills:** The candidate should work on organizing and structuring their responses to ensure they are clear and coherent, which would involve providing specific examples and elaborating on points made.\n",
            "\n",
            "4. **Technical Competency: 1/10:**\n",
            "   The candidate displayed minimal technical knowledge pertinent to the data science role. There were no substantial references to specific methodologies, tools, or examples that would demonstrate competence in data handling, analysis, or visualization.\n",
            "\n",
            "5. **Communication Skills: 1/10:**\n",
            "   The communication skills demonstrated were insufficient for a data science role. The responses were brief, lacked structure, and did not adequately communicate understanding or sufficient detail to inform the interviewer.\n",
            "\n",
            "6. **Recommendation: Do Not Recommend**\n",
            "   Based on the demonstrated lack of technical knowledge, inadequate communication skills, and the inability to provide relevant responses or examples, it is not recommended to proceed with this candidate for the data science role. Significant improvement and preparation would be necessary before a reconsideration for such a position.\n",
            "\n",
            "Interview Summary:\n",
            "**Overall Assessment:**\n",
            "\n",
            "The candidate's performance in this interview was significantly lacking in several key areas essential for a data science role. Across multiple questions, the candidate failed to demonstrate a sufficient understanding of technical concepts, such as asynchronous operations in Node.js, performance optimization, task prioritization, and data visualization strategies. The candidate's responses were often brief, vague, and off-topic, showing a lack of depth and practical application understanding. This suggests either insufficient preparation or a gap in fundamental knowledge necessary for effectively applying data science principles in real-world scenarios.\n",
            "\n",
            "**Key Strengths:**\n",
            "\n",
            "It is challenging to identify specific strengths based on the provided responses since consistent patterns of strengths were not evident throughout the interview. Most answers lacked depth, specificity, and relevance to the posed questions.\n",
            "\n",
            "**Areas for Improvement:**\n",
            "\n",
            "1. **Technical Knowledge and Depth:** The candidate should focus on acquiring and showcasing solid foundational knowledge in data science topics, including data handling, EDA, visualization, and technical problem-solving. More comprehensive and relevant responses are necessary.\n",
            "   \n",
            "2. **Relevance and Structure in Responses:** Answers should address the questions directly, presenting structured thoughts and tying them back to practical examples wherever possible.\n",
            "\n",
            "3. **Practical Application:** The candidate would benefit from gaining hands-on experience and learning how to tie theoretical concepts to real-world scenarios clearly and effectively.\n",
            "\n",
            "**Technical Competency:**\n",
            "\n",
            "3/10 - The candidate exhibited minimal technical knowledge in the areas questioned, showing little to no understanding of core data science principles and methodologies.\n",
            "\n",
            "**Communication Skills:**\n",
            "\n",
            "2/10 - The candidate's communication was lacking in clarity, structure, and relevance, making it difficult to follow their responses or assess their knowledge effectively.\n",
            "\n",
            "**Recommendation:**\n",
            "\n",
            "Do Not Recommend - The candidate does not currently demonstrate the necessary skills or knowledge base required for a data science role. Considerable improvement in technical understanding and communication is needed before the candidate would be ready for this position. Further study, practical application, and perhaps mentorship could benefit their future candidacy.\n",
            "Took 7.61 seconds\n",
            "\n",
            "\n",
            "Interview results saved to interview_results_20250417-011303.json\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install python-docx\n",
        "!pip install pandas\n",
        "!pip install openai\n",
        "!pip install pydub\n",
        "!pip install google-cloud-speech\n",
        "!pip install google-cloud-storage\n",
        "!pip install python-dotenv\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain_openai\n",
        "!pip install chromadb\n",
        "\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import io\n",
        "import numpy as np\n",
        "import urllib.parse\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# File handling and parsing libraries\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# OpenAI integration\n",
        "from openai import OpenAI\n",
        "\n",
        "# Audio processing\n",
        "from pydub import AudioSegment\n",
        "from google.cloud import speech_v1p1beta1 as speech\n",
        "from google.cloud import storage\n",
        "\n",
        "# RAG components\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "#----------------------------------------\n",
        "# UTILITY FUNCTIONS\n",
        "#----------------------------------------\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Generic function to read file content based on extension\"\"\"\n",
        "    try:\n",
        "        # Determine file type based on extension\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        # Read the file content based on type\n",
        "        if file_extension == '.pdf':\n",
        "            return read_pdf(file_path)\n",
        "        elif file_extension == '.docx':\n",
        "            return read_docx(file_path)\n",
        "        elif file_extension == '.txt':\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "        elif file_extension in ['.csv', '.xlsx', '.xls']:\n",
        "            # For tabular formats, convert to text representation\n",
        "            return read_tabular(file_path)\n",
        "        else:\n",
        "            # Try to read as text for unknown file types\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
        "        return f\"Could not read file {file_path}\"\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def read_docx(docx_path):\n",
        "    \"\"\"Extract text from DOCX file\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(docx_path)\n",
        "        text = []\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text.append(paragraph.text)\n",
        "        return \"\\n\".join(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading DOCX: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def read_tabular(file_path):\n",
        "    \"\"\"Extract text representation from tabular files (CSV, Excel)\"\"\"\n",
        "    try:\n",
        "        extension = os.path.splitext(file_path)[1].lower()\n",
        "        if extension == '.csv':\n",
        "            df = pd.read_csv(file_path)\n",
        "        else:  # Excel formats\n",
        "            df = pd.read_excel(file_path)\n",
        "\n",
        "        # Convert DataFrame to text representation\n",
        "        text = \"Resume in tabular format:\\n\\n\"\n",
        "        for col in df.columns:\n",
        "            text += f\"{col}:\\n\"\n",
        "            for val in df[col].dropna():\n",
        "                text += f\"- {val}\\n\"\n",
        "            text += \"\\n\"\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading tabular file: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def convert_https_to_gcs_uri(https_url):\n",
        "    \"\"\"Convert HTTPS URL to Google Cloud Storage URI\"\"\"\n",
        "    try:\n",
        "        # Parse the URL\n",
        "        parsed_url = urllib.parse.urlparse(https_url)\n",
        "        # Extract bucket name and object name\n",
        "        path_parts = parsed_url.path.lstrip('/').split('/', 1)\n",
        "        if len(path_parts) == 2:\n",
        "            bucket_name, object_name = path_parts\n",
        "            # Decode URL-encoded characters in the object name\n",
        "            object_name = urllib.parse.unquote(object_name)\n",
        "            return f\"gs://{bucket_name}/{object_name}\"\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid HTTPS URL format: {https_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting URL to GCS URI: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "#----------------------------------------\n",
        "# RESUME AND JOB DESCRIPTION ANALYSIS\n",
        "#----------------------------------------\n",
        "\n",
        "def parse_resume_with_ai(resume_text):\n",
        "    \"\"\"Use AI to extract structured information from resume text\"\"\"\n",
        "    try:\n",
        "        load_dotenv()\n",
        "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Extract structured information from the following resume:\n",
        "\n",
        "        {resume_text}\n",
        "\n",
        "        Return the information in this JSON format:\n",
        "        {{\n",
        "            \"contact_info\": {{\n",
        "                \"name\": \"\",\n",
        "                \"email\": \"\",\n",
        "                \"phone\": \"\",\n",
        "                \"location\": \"\"\n",
        "            }},\n",
        "            \"summary\": \"\",\n",
        "            \"education\": [\n",
        "                {{\n",
        "                    \"degree\": \"\",\n",
        "                    \"institution\": \"\",\n",
        "                    \"dates\": \"\",\n",
        "                    \"gpa\": \"\",\n",
        "                    \"details\": []\n",
        "                }}\n",
        "            ],\n",
        "            \"experience\": [\n",
        "                {{\n",
        "                    \"title\": \"\",\n",
        "                    \"company\": \"\",\n",
        "                    \"dates\": \"\",\n",
        "                    \"location\": \"\",\n",
        "                    \"responsibilities\": []\n",
        "                }}\n",
        "            ],\n",
        "            \"skills\": {{\n",
        "                \"technical\": [],\n",
        "                \"soft\": [],\n",
        "                \"languages\": [],\n",
        "                \"tools\": []\n",
        "            }},\n",
        "            \"projects\": [\n",
        "                {{\n",
        "                    \"name\": \"\",\n",
        "                    \"description\": \"\",\n",
        "                    \"technologies\": [],\n",
        "                    \"outcomes\": []\n",
        "                }}\n",
        "            ],\n",
        "            \"certifications\": [],\n",
        "            \"years_of_experience\": 0,\n",
        "            \"domain_expertise\": []\n",
        "        }}\n",
        "\n",
        "        If a field can't be determined from the resume, use null or an empty array as appropriate.\n",
        "        For \"years_of_experience\", determine the total years of professional experience based on work history.\n",
        "        For \"domain_expertise\", identify industry domains where the candidate has experience.\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert resume parser. Extract structured information from resumes accurately.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        start = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        structured_resume = json.loads(response.choices[0].message.content)\n",
        "        print(f\"Resume parsed in {round(end - start, 2)} seconds\")\n",
        "\n",
        "        return structured_resume\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing resume with AI: {str(e)}\")\n",
        "        return {\n",
        "            \"contact_info\": {},\n",
        "            \"education\": [],\n",
        "            \"experience\": [],\n",
        "            \"skills\": {},\n",
        "            \"years_of_experience\": 0,\n",
        "            \"domain_expertise\": []\n",
        "        }\n",
        "\n",
        "def extract_resume_structure(resume_path):\n",
        "    \"\"\"Extract structured information from a resume file\"\"\"\n",
        "    try:\n",
        "        # Read the file content\n",
        "        resume_text = read_file(resume_path)\n",
        "\n",
        "        # Use OpenAI to extract structured information\n",
        "        structured_resume = parse_resume_with_ai(resume_text)\n",
        "        return structured_resume\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting resume structure: {str(e)}\")\n",
        "        # Return a minimal structure if parsing fails\n",
        "        return {\n",
        "            \"contact_info\": {},\n",
        "            \"education\": [],\n",
        "            \"experience\": [],\n",
        "            \"skills\": [],\n",
        "            \"full_text\": \"Could not parse resume\"\n",
        "        }\n",
        "\n",
        "def extract_job_description_structure(jd_text):\n",
        "    \"\"\"Extract structured information from job description text\"\"\"\n",
        "    try:\n",
        "        load_dotenv()\n",
        "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Extract structured information from the following job description:\n",
        "\n",
        "        {jd_text}\n",
        "\n",
        "        Return the information in this JSON format:\n",
        "        {{\n",
        "            \"title\": \"\",\n",
        "            \"company\": \"\",\n",
        "            \"location\": \"\",\n",
        "            \"employment_type\": \"\",\n",
        "            \"summary\": \"\",\n",
        "            \"responsibilities\": [],\n",
        "            \"requirements\": {{\n",
        "                \"required_skills\": [],\n",
        "                \"preferred_skills\": [],\n",
        "                \"education\": [],\n",
        "                \"experience\": []\n",
        "            }},\n",
        "            \"benefits\": [],\n",
        "            \"industry\": \"\",\n",
        "            \"seniority_level\": \"\",\n",
        "            \"key_technologies\": []\n",
        "        }}\n",
        "\n",
        "        If a field can't be determined from the job description, use null or an empty array as appropriate.\n",
        "        For \"industry\", identify the industry sector this role belongs to.\n",
        "        For \"seniority_level\", determine if this is entry-level, mid-level, senior, or executive.\n",
        "        For \"key_technologies\", identify specific technologies, tools, or platforms mentioned as requirements.\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert job description parser. Extract structured information from job descriptions accurately.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        start = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        structured_jd = json.loads(response.choices[0].message.content)\n",
        "        print(f\"Job description parsed in {round(end - start, 2)} seconds\")\n",
        "\n",
        "        return structured_jd\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing job description with AI: {str(e)}\")\n",
        "        return {\n",
        "            \"title\": \"\",\n",
        "            \"company\": \"\",\n",
        "            \"responsibilities\": [],\n",
        "            \"requirements\": {\"required_skills\": []},\n",
        "            \"industry\": \"\",\n",
        "            \"seniority_level\": \"\"\n",
        "        }\n",
        "\n",
        "def analyze_resume_job_alignment(structured_resume, structured_jd):\n",
        "    \"\"\"Analyze the alignment between resume and job description\"\"\"\n",
        "    try:\n",
        "        load_dotenv()\n",
        "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        # Prepare the input for analysis\n",
        "        resume_json = json.dumps(structured_resume, indent=2)\n",
        "        jd_json = json.dumps(structured_jd, indent=2)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the alignment between this candidate's resume and the job description:\n",
        "\n",
        "        RESUME:\n",
        "        {resume_json}\n",
        "\n",
        "        JOB DESCRIPTION:\n",
        "        {jd_json}\n",
        "\n",
        "        Provide a JSON analysis with the following structure:\n",
        "        {{\n",
        "            \"overall_match_score\": 0-100,\n",
        "            \"skill_match\": {{\n",
        "                \"score\": 0-100,\n",
        "                \"matching_skills\": [],\n",
        "                \"missing_skills\": []\n",
        "            }},\n",
        "            \"experience_match\": {{\n",
        "                \"score\": 0-100,\n",
        "                \"relevant_experience\": [],\n",
        "                \"experience_gaps\": []\n",
        "            }},\n",
        "            \"education_match\": {{\n",
        "                \"score\": 0-100,\n",
        "                \"comments\": \"\"\n",
        "            }},\n",
        "            \"strengths\": [],\n",
        "            \"areas_of_concern\": [],\n",
        "            \"suggested_interview_focus_areas\": []\n",
        "        }}\n",
        "\n",
        "        Base your analysis on how well the candidate's qualifications align with the job requirements.\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert resume analyzer with deep knowledge of job requirements and candidate evaluation.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        start = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        alignment_analysis = json.loads(response.choices[0].message.content)\n",
        "        print(f\"Resume-job alignment analyzed in {round(end - start, 2)} seconds\")\n",
        "\n",
        "        return alignment_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing resume-job alignment: {str(e)}\")\n",
        "        return {\n",
        "            \"overall_match_score\": 50,\n",
        "            \"strengths\": [\"Could not perform detailed analysis\"],\n",
        "            \"areas_of_concern\": [\"Could not perform detailed analysis\"],\n",
        "            \"suggested_interview_focus_areas\": [\"Technical skills\", \"Experience\", \"Problem-solving\"]\n",
        "        }\n",
        "\n",
        "#----------------------------------------\n",
        "# AUDIO PROCESSING FUNCTIONS\n",
        "#----------------------------------------\n",
        "\n",
        "def upload_to_gcp_storage(file_data, bucket_name):\n",
        "    \"\"\"Upload file to Google Cloud Storage\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        timestamp = int(time.time())\n",
        "        blob_name = f\"recording_{timestamp}.webm\"\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        # Upload the file\n",
        "        blob.upload_from_string(file_data.read(), content_type=\"audio/webm\")\n",
        "\n",
        "        # Generate a publicly accessible URL\n",
        "        return f\"https://storage.googleapis.com/{bucket_name}/{blob_name}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading to GCP: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def speech_to_text(gcs_uri):\n",
        "    \"\"\"Convert speech to text using Google Speech-to-Text API\"\"\"\n",
        "    try:\n",
        "        client = speech.SpeechClient()\n",
        "\n",
        "        # Configure the recognition request\n",
        "        audio = speech.RecognitionAudio(uri=gcs_uri)\n",
        "        config = speech.RecognitionConfig(\n",
        "            encoding=speech.RecognitionConfig.AudioEncoding.WEBM_OPUS,\n",
        "            sample_rate_hertz=48000,\n",
        "            language_code='en-US',\n",
        "            enable_automatic_punctuation=True,\n",
        "            audio_channel_count=1,\n",
        "            enable_word_time_offsets=True\n",
        "        )\n",
        "\n",
        "        # Use long_running_recognize for files longer than 1 minute\n",
        "        operation = client.long_running_recognize(config=config, audio=audio)\n",
        "        print(\"Waiting for speech-to-text operation to complete...\")\n",
        "        response = operation.result(timeout=90)\n",
        "\n",
        "        # Process the response\n",
        "        transcript = \"\"\n",
        "        for result in response.results:\n",
        "            transcript += result.alternatives[0].transcript + \" \"\n",
        "\n",
        "        return transcript.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in speech-to-text: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_audio_features(gcs_uri):\n",
        "    \"\"\"Extract audio features from a WebM file\"\"\"\n",
        "    try:\n",
        "        # Load audio from GCS\n",
        "        bucket_name = gcs_uri.split('/')[2]\n",
        "        blob_name = '/'.join(gcs_uri.split('/')[3:])\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        audio_bytes = io.BytesIO()\n",
        "        blob.download_to_file(audio_bytes)\n",
        "        audio_bytes.seek(0)\n",
        "\n",
        "        # Load the audio using pydub\n",
        "        webm_audio = AudioSegment.from_file(audio_bytes, format=\"webm\")\n",
        "\n",
        "        # Convert to numpy array for processing\n",
        "        samples = np.array(webm_audio.get_array_of_samples()).astype(np.float64)\n",
        "        if webm_audio.channels == 2:\n",
        "            samples = samples.reshape((-1, 2)).mean(axis=1)\n",
        "\n",
        "        # Extract basic audio features (simplified version)\n",
        "        audio_duration = webm_audio.duration_seconds\n",
        "        sample_rate = webm_audio.frame_rate\n",
        "\n",
        "        # Calculate audio intensity (volume)\n",
        "        avg_volume = np.mean(np.abs(samples))\n",
        "\n",
        "        # Calculate pauses (periods of relative silence)\n",
        "        silence_threshold = avg_volume * 0.1\n",
        "        is_silence = np.abs(samples) < silence_threshold\n",
        "        silence_ranges = []\n",
        "        in_silence = False\n",
        "        silence_start = 0\n",
        "\n",
        "        for i, silent in enumerate(is_silence):\n",
        "            if silent and not in_silence:\n",
        "                in_silence = True\n",
        "                silence_start = i\n",
        "            elif not silent and in_silence:\n",
        "                in_silence = False\n",
        "                silence_duration = (i - silence_start) / sample_rate\n",
        "                if silence_duration >= 0.5:  # Only count pauses >= 0.5 seconds\n",
        "                    silence_ranges.append(silence_duration)\n",
        "\n",
        "        # Calculate features\n",
        "        features = {\n",
        "            'audio_length': float(audio_duration),\n",
        "            'avg_volume': float(avg_volume),\n",
        "            'max_pause': float(max(silence_ranges)) if silence_ranges else 0.0,\n",
        "            'avg_pause': float(np.mean(silence_ranges)) if silence_ranges else 0.0,\n",
        "            'num_pauses': len(silence_ranges)\n",
        "        }\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting audio features: {str(e)}\")\n",
        "        return {\n",
        "            'audio_length': 0.0,\n",
        "            'avg_volume': 0.0,\n",
        "            'max_pause': 0.0,\n",
        "            'avg_pause': 0.0,\n",
        "            'num_pauses': 0\n",
        "        }\n",
        "\n",
        "def process_audio_recording(webm_file):\n",
        "    \"\"\"Process audio recording and extract transcript\"\"\"\n",
        "    try:\n",
        "        print(\"Processing WebM file\")\n",
        "\n",
        "        # Save WebM file to GCP storage\n",
        "        webm_url = upload_to_gcp_storage(webm_file, \"lia_recordings\")\n",
        "\n",
        "        # Convert HTTPS URL to GCS URI\n",
        "        gcs_uri = convert_https_to_gcs_uri(webm_url)\n",
        "        print(f\"GCS URI: {gcs_uri}\")\n",
        "\n",
        "        # Get transcript using Google Speech-to-Text\n",
        "        transcript = speech_to_text(gcs_uri)\n",
        "\n",
        "        # Extract audio features\n",
        "        audio_features = extract_audio_features(gcs_uri)\n",
        "\n",
        "        return {\n",
        "            'transcript': transcript,\n",
        "            'audio_url': webm_url,\n",
        "            'audio_features': audio_features\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing audio recording: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "#----------------------------------------\n",
        "# TEXT FEATURE EXTRACTION\n",
        "#----------------------------------------\n",
        "\n",
        "class TextFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Filler words list\n",
        "        self.filler_words = [\n",
        "            \"um\", \"uhm\", \"uh\", \"uhh\", \"er\", \"err\", \"ah\", \"ahh\", \"like\", \"you know\",\n",
        "            \"sort of\", \"kind of\", \"kinda\", \"sorta\", \"basically\", \"literally\", \"actually\",\n",
        "            \"so\", \"well\", \"I mean\", \"you see\", \"right\", \"okay\", \"ok\", \"yeah\", \"yea\",\n",
        "            \"you know what I mean\", \"I guess\", \"just\", \"stuff\", \"things\", \"whatever\"\n",
        "        ]\n",
        "\n",
        "        # Quantifier words list\n",
        "        self.quantifiers = [\n",
        "            \"all\", \"another\", \"any\", \"both\", \"each\", \"either\", \"enough\", \"every\",\n",
        "            \"few\", \"fewer\", \"little\", \"less\", \"many\", \"more\", \"much\", \"neither\",\n",
        "            \"no\", \"several\", \"some\", \"a few\", \"a little\", \"a lot of\", \"lots of\",\n",
        "            \"most\", \"plenty\", \"numerous\", \"countless\"\n",
        "        ]\n",
        "\n",
        "        # Compile regex patterns\n",
        "        self.filler_pattern = r'\\b(' + '|'.join(map(re.escape, self.filler_words)) + r')\\b'\n",
        "        self.quantifier_pattern = r'\\b(' + '|'.join(map(re.escape, self.quantifiers)) + r')\\b'\n",
        "\n",
        "    def extract_features(self, text, audio_length):\n",
        "        \"\"\"Extract text features from the given text\"\"\"\n",
        "        try:\n",
        "            # Convert text to lowercase for consistent matching\n",
        "            text = text.lower()\n",
        "\n",
        "            # Count total words (splitting on whitespace)\n",
        "            total_words = len(text.split())\n",
        "\n",
        "            # Count unique words\n",
        "            unique_words = len(set(text.split()))\n",
        "\n",
        "            # Count filler words\n",
        "            filler_count = len(re.findall(self.filler_pattern, text))\n",
        "\n",
        "            # Count quantifier words\n",
        "            quantifier_count = len(re.findall(self.quantifier_pattern, text))\n",
        "\n",
        "            # Calculate percentages (avoid division by zero)\n",
        "            filler_pct = (filler_count / total_words * 100) if total_words > 0 else 0\n",
        "            quantifier_pct = (quantifier_count / total_words * 100) if total_words > 0 else 0\n",
        "\n",
        "            # Calculate words per second and unique words per second\n",
        "            wpsec = (total_words / audio_length) if audio_length > 0 else 0\n",
        "            upsec = (unique_words / audio_length) if audio_length > 0 else 0\n",
        "\n",
        "            return {\n",
        "                'quantifier_words_pct': float(quantifier_pct),\n",
        "                'filler_nonfluency_pct': float(filler_pct),\n",
        "                'word_count': total_words,\n",
        "                'wpsec': float(wpsec),\n",
        "                'upsec': float(upsec)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text features: {str(e)}\")\n",
        "            return {\n",
        "                'quantifier_words_pct': 0.0,\n",
        "                'filler_nonfluency_pct': 0.0,\n",
        "                'word_count': 0,\n",
        "                'wpsec': 0.0,\n",
        "                'upsec': 0.0\n",
        "            }\n",
        "\n",
        "#----------------------------------------\n",
        "# RAG (RETRIEVAL AUGMENTED GENERATION)\n",
        "#----------------------------------------\n",
        "\n",
        "def initialize_rag(file_path):\n",
        "    \"\"\"Initialize Retrieval-Augmented Generation\"\"\"\n",
        "    try:\n",
        "        print(\"Initializing RAG with knowledge base...\")\n",
        "        loader = TextLoader(file_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        # Initialize OpenAI embeddings\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        embeddings = OpenAIEmbeddings(api_key=api_key)\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        docs = text_splitter.split_documents(documents)\n",
        "\n",
        "        vector_db = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "        return vector_db\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing RAG: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def setup_retrieval_qa(vector_db):\n",
        "    \"\"\"Set up the retrieval QA system\"\"\"\n",
        "    try:\n",
        "        if not vector_db:\n",
        "            return None\n",
        "\n",
        "        retriever = vector_db.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 4}\n",
        "        )\n",
        "\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        llm = ChatOpenAI(\n",
        "            model_name=\"gpt-4o\",\n",
        "            api_key=api_key,\n",
        "            temperature=0.1,\n",
        "            max_tokens=2000\n",
        "        )\n",
        "\n",
        "        qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "\n",
        "        return qa\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up retrieval QA: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "#----------------------------------------\n",
        "# QUESTION GENERATION\n",
        "#----------------------------------------\n",
        "\n",
        "def generate_initial_question(structured_resume, structured_jd):\n",
        "    \"\"\"Generate the initial interview question based on structured resume and job description\"\"\"\n",
        "    # Prepare resume and job description in a structured format\n",
        "    resume_summary = {\n",
        "        \"name\": structured_resume.get(\"contact_info\", {}).get(\"name\", \"\"),\n",
        "        \"experience\": [\n",
        "            {\n",
        "                \"title\": exp.get(\"title\", \"\"),\n",
        "                \"company\": exp.get(\"company\", \"\"),\n",
        "                \"responsibilities\": exp.get(\"responsibilities\", [])[:3]  # Limit to 3 responsibilities\n",
        "            } for exp in structured_resume.get(\"experience\", [])[:3]  # Limit to 3 most recent experiences\n",
        "        ],\n",
        "        \"education\": [\n",
        "            {\n",
        "                \"degree\": edu.get(\"degree\", \"\"),\n",
        "                \"institution\": edu.get(\"institution\", \"\")\n",
        "            } for edu in structured_resume.get(\"education\", [])\n",
        "        ],\n",
        "        \"skills\": structured_resume.get(\"skills\", {}),\n",
        "        \"projects\": [proj.get(\"name\", \"\") for proj in structured_resume.get(\"projects\", [])[:3]]\n",
        "    }\n",
        "\n",
        "    jd_summary = {\n",
        "        \"title\": structured_jd.get(\"title\", \"\"),\n",
        "        \"company\": structured_jd.get(\"company\", \"\"),\n",
        "        \"responsibilities\": structured_jd.get(\"responsibilities\", [])[:5],  # Limit to 5 key responsibilities\n",
        "        \"required_skills\": structured_jd.get(\"requirements\", {}).get(\"required_skills\", []),\n",
        "        \"industry\": structured_jd.get(\"industry\", \"\")\n",
        "    }\n",
        "\n",
        "    # Convert to JSON strings\n",
        "    resume_json = json.dumps(resume_summary, indent=2)\n",
        "    jd_json = json.dumps(jd_summary, indent=2)\n",
        "\n",
        "    # Prepare messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert AI interviewer for data science roles.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Given the structured resume and job description below, generate 1 thoughtful and role-specific technical interview question.\n",
        "\n",
        "STRUCTURED RESUME:\n",
        "{resume_json}\n",
        "\n",
        "STRUCTURED JOB DESCRIPTION:\n",
        "{jd_json}\n",
        "\n",
        "Guidelines for your question:\n",
        "1. Be specific to the candidate's background and the job requirements\n",
        "2. Focus on technical knowledge and practical application\n",
        "3. Target skills or experiences that appear most relevant for this role\n",
        "4. Keep the question under 30 words and end with a question mark\n",
        "5. Make it conversational but substantive\n",
        "\"\"\"}\n",
        "    ]\n",
        "\n",
        "    # Call OpenAI API\n",
        "    load_dotenv()\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    start = time.time()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "    end = time.time()\n",
        "\n",
        "    initial_question = response.choices[0].message.content\n",
        "    print(f\"\\nInitial Question:\\n{initial_question}\")\n",
        "    print(f\"Took {round(end - start, 2)} seconds\\n\")\n",
        "\n",
        "    return initial_question, messages\n",
        "\n",
        "def generate_follow_up_questions(messages):\n",
        "    \"\"\"Generate follow-up questions based on the initial question\"\"\"\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"\"\"\n",
        "Based on the initial interview question generated above, generate 3 additional follow-up questions that are related to those questions.\n",
        "Each question should:\n",
        "1. Be clearly distinct from the others\n",
        "2. Probe different aspects of the candidate's knowledge and experience\n",
        "3. Follow a logical progression of complexity\n",
        "4. Be specific to the data science role indicated in the context\n",
        "5. Be formatted as numbered questions (1., 2., 3.)\n",
        "\"\"\"})\n",
        "\n",
        "    load_dotenv()\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    start = time.time()\n",
        "    followup_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "    end = time.time()\n",
        "\n",
        "    follow_up_questions = followup_response.choices[0].message.content\n",
        "    print(f\"Generated follow-up questions:\\n{follow_up_questions}\")\n",
        "    print(f\"Took {round(end - start, 2)} seconds\\n\")\n",
        "\n",
        "    # Extract the questions from the response\n",
        "    questions = []\n",
        "    for line in follow_up_questions.strip().split('\\n'):\n",
        "        if re.match(r'^\\d+\\.', line.strip()):\n",
        "            questions.append(line.strip())\n",
        "\n",
        "    return questions\n",
        "\n",
        "def generate_tailored_questions(personal_profile):\n",
        "    \"\"\"Generate interview questions tailored to the candidate's background and role\"\"\"\n",
        "    try:\n",
        "        load_dotenv()\n",
        "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Generate 5 tailored technical interview questions for a {personal_profile['role']} position based on the candidate's background:\n",
        "\n",
        "        Candidate Profile:\n",
        "        - Experience: {personal_profile['experience']} years\n",
        "        - Education: {', '.join(personal_profile['education'])}\n",
        "        - Technical Skills: {', '.join(personal_profile['skills']['technical'][:10])}\n",
        "        - Domain Expertise: {', '.join(personal_profile['domain_expertise'])}\n",
        "\n",
        "        Job Requirements:\n",
        "        - Role: {personal_profile['role']} at {personal_profile['company']}\n",
        "        - Industry: {personal_profile['industry']}\n",
        "        - Seniority: {personal_profile['seniority_level']}\n",
        "        - Key Technologies: {', '.join(personal_profile['key_technologies'])}\n",
        "        - Key Responsibilities: {', '.join(personal_profile['job_responsibilities'][:3])}\n",
        "\n",
        "        Generate 5 different types of questions:\n",
        "        1. A technical knowledge question about a key technology required for the role\n",
        "        2. A problem-solving question related to the industry challenges\n",
        "        3. A scenario-based question about handling a specific job responsibility\n",
        "        4. A question about a project mentioned in their background related to the job requirements\n",
        "        5. A question that tests the candidate's understanding of how their skills apply to the company's industry\n",
        "\n",
        "        Each question should:\n",
        "        - Be specific to this candidate and role (not generic)\n",
        "        - Be 1-2 sentences long\n",
        "        - Probe for both knowledge and applied experience\n",
        "        - Not require external resources to answer\n",
        "        - End with a question mark\n",
        "\n",
        "        Format each question as:\n",
        "        \"1. [Technical Knowledge] Question text\"\n",
        "        \"2. [Problem Solving] Question text\"\n",
        "        etc.\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert technical interviewer for data science roles, with deep knowledge of industry best practices and technical requirements.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        start = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        questions = response.choices[0].message.content.strip().split(\"\\n\")\n",
        "        print(f\"Generated {len(questions)} tailored questions in {round(end - start, 2)} seconds\")\n",
        "\n",
        "        # Filter out any non-question lines and clean up formatting\n",
        "        filtered_questions = []\n",
        "        for question in questions:\n",
        "            # Check if line starts with a number and contains a question mark\n",
        "            if re.match(r'^\\d+\\.', question) and '?' in question:\n",
        "                # Extract the question text, removing the category label if present\n",
        "                question_text = re.sub(r'^\\d+\\.\\s+\\[[^\\]]+\\]\\s+', '', question)\n",
        "                filtered_questions.append(question_text)\n",
        "\n",
        "        return filtered_questions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating tailored questions: {str(e)}\")\n",
        "        return [\n",
        "            \"Tell me about your experience with data science projects.\",\n",
        "            \"How would you approach a classification problem?\",\n",
        "            \"What programming languages are you most comfortable with?\",\n",
        "            \"Describe a challenging data problem you've solved.\",\n",
        "            \"How do you stay updated with the latest developments in data science?\"\n",
        "        ]\n",
        "\n",
        "def generate_dynamic_question(messages, user_response, retrieval_qa=None, personal_profile=None):\n",
        "    \"\"\"Generate a dynamic follow-up question based on the user's response\"\"\"\n",
        "    # Add the user's response to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": user_response})\n",
        "\n",
        "    # Try to use RAG-enhanced question generation if available\n",
        "    if retrieval_qa and personal_profile:\n",
        "        try:\n",
        "            # Create a context prompt that includes the conversation history\n",
        "            context_prompt = f\"\"\"\n",
        "            Based on this interview conversation so far and the candidate's profile:\n",
        "\n",
        "            Profile: {personal_profile}\n",
        "\n",
        "            Last Response: {user_response}\n",
        "\n",
        "            Generate a follow-up interview question that:\n",
        "            1. Builds directly on the candidate's previous answer\n",
        "            2. Is under 35 words\n",
        "            3. Is phrased directly as a question\n",
        "            4. Probes deeper into technical knowledge or experience\n",
        "            \"\"\"\n",
        "\n",
        "            # Get the response with source documents\n",
        "            context_response = retrieval_qa.retriever.vectorstore.similarity_search_with_relevance_scores(context_prompt)\n",
        "\n",
        "            # Check if we have good matches\n",
        "            use_rag = False\n",
        "            if len(context_response) > 1:\n",
        "                avg_similarity = sum(score for _, score in context_response) / len(context_response)\n",
        "                use_rag = avg_similarity >= 0.65\n",
        "                print(f\"Found {len(context_response)} chunks with average similarity score: {avg_similarity}\")\n",
        "\n",
        "            if use_rag:\n",
        "                print(\"Using RAG knowledge base for question generation\")\n",
        "                response = retrieval_qa({\"query\": context_prompt})\n",
        "                follow_up_question = response['result'].strip()\n",
        "                return follow_up_question\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error using RAG for question generation: {str(e)}\")\n",
        "\n",
        "    # Fall back to OpenAI if RAG fails or is not available\n",
        "    print(\"Using OpenAI for question generation\")\n",
        "\n",
        "    load_dotenv()\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Create a new system message\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "            You are an expert technical interviewer for data science roles.\n",
        "            Generate a thoughtful follow-up question based on the candidate's response.\n",
        "            The question should:\n",
        "            1. Dig deeper into a specific aspect mentioned by the candidate\n",
        "            2. Be conversational but technically substantive\n",
        "            3. Be no longer than 30 words\n",
        "            4. End with a question mark\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    # Create a new messages array with the system message first\n",
        "    formatted_messages = [system_message] + messages\n",
        "\n",
        "    start = time.time()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=formatted_messages,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    end = time.time()\n",
        "\n",
        "    follow_up_question = response.choices[0].message.content\n",
        "    print(f\"\\nDynamic Follow-Up Question: {follow_up_question}\")\n",
        "    print(f\"Took {round(end - start, 2)} seconds\\n\")\n",
        "\n",
        "    return follow_up_question\n",
        "\n",
        "#----------------------------------------\n",
        "# ANSWER EVALUATION\n",
        "#----------------------------------------\n",
        "\n",
        "def generate_expert_answer(question, retrieval_qa=None, personal_profile=None):\n",
        "    \"\"\"Generate an expert answer to the interview question\"\"\"\n",
        "    if not retrieval_qa or not personal_profile:\n",
        "        return \"Expert answers require a retrieval QA system and personal profile.\"\n",
        "\n",
        "    template = f\"\"\"\n",
        "    You are an experienced data scientist in a job interview. Answer the following question naturally as if speaking to the interviewer, using your provided personal profile for context.\n",
        "\n",
        "    Context:\n",
        "    Personal Profile: {personal_profile}\n",
        "    Question: {question}\n",
        "\n",
        "    Guidelines for your response:\n",
        "    - Speak in first person (\"I\", \"my\", \"we\")\n",
        "    - Use natural transitions and connecting phrases\n",
        "    - Keep it concise (150-200 words)\n",
        "    - Include a brief opening statement that directly answers the question\n",
        "    - Follow with a technical explanation using correct terminology\n",
        "    - Share a specific example from your personal profile\n",
        "    - Conclude with the business impact or practical application\n",
        "\n",
        "    Response style:\n",
        "    - Conversational and engaging, as if speaking in person\n",
        "    - Professional but approachable\n",
        "    - Show both technical expertise and communication skills\n",
        "    - Avoid formal documentation style or academic tone\n",
        "    - No bullet points or formatting - use natural speech flow\n",
        "\n",
        "    Example flow:\n",
        "    \"In my experience... [direct answer]. This works because... [technical explanation]. For instance, when I was at... [personal example]. This approach helped us... [business impact]\"\n",
        "\n",
        "    Remember: You're having a conversation, not writing a report. Make it engaging while showcasing your expertise.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # First, try to get relevant chunks and check similarity\n",
        "        context_response = retrieval_qa.retriever.vectorstore.similarity_search_with_relevance_scores(template)\n",
        "\n",
        "        # Set similarity score threshold\n",
        "        SIMILARITY_THRESHOLD = 0.65\n",
        "\n",
        "        # Calculate average similarity if we have chunks\n",
        "        use_rag = False\n",
        "        if len(context_response) > 1:\n",
        "            avg_similarity = sum(score for _, score in context_response) / len(context_response)\n",
        "            use_rag = avg_similarity >= SIMILARITY_THRESHOLD\n",
        "            print(f\"Found {len(context_response)} chunks with average similarity score: {avg_similarity}\")\n",
        "\n",
        "        if use_rag:\n",
        "            print(\"Generating expert answer using RAG knowledge base\")\n",
        "            answer = retrieval_qa({\"query\": template})\n",
        "            return answer['result']\n",
        "\n",
        "        # If similarity is too low, use OpenAI directly\n",
        "        print(\"Generating expert answer using GPT-4o\")\n",
        "        load_dotenv()\n",
        "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an experienced data scientist in a job interview.\"},\n",
        "            {\"role\": \"user\", \"content\": template}\n",
        "        ]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            temperature=0.1,\n",
        "            max_tokens=2000\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating expert answer: {str(e)}\")\n",
        "        return \"Could not generate expert answer due to an error.\"\n",
        "\n",
        "def evaluate_answer(user_answer, expert_answer, question, audio_features=None, text_features=None):\n",
        "    \"\"\"Evaluate the user's answer compared to the expert answer\"\"\"\n",
        "\n",
        "    evaluation_prompt = f\"\"\"\n",
        "    You are an expert interviewer and evaluator for data science roles. Compare the candidate's answer to an expert reference answer for the same question.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Candidate's Answer: {user_answer}\n",
        "\n",
        "    Expert Reference Answer: {expert_answer}\n",
        "\n",
        "    Audio Features: {audio_features if audio_features else 'Not available'}\n",
        "\n",
        "    Text Features: {text_features if text_features else 'Not available'}\n",
        "\n",
        "    Evaluate the candidate's answer on the following criteria:\n",
        "    1. Technical Accuracy (0-10): How well does the answer demonstrate understanding of technical concepts?\n",
        "    2. Relevance (0-10): How directly does the answer address the question asked?\n",
        "    3. Depth (0-10): How well does the answer explore the topic beyond surface level?\n",
        "    4. Communication (0-10): How clear, concise, and well-structured is the answer?\n",
        "    5. Practical Application (0-10): How well does the answer connect to real-world applications?\n",
        "\n",
        "    For each criterion, provide:\n",
        "    - A numerical score\n",
        "    - 1-2 sentences of specific feedback\n",
        "    - A specific improvement suggestion\n",
        "\n",
        "    Then provide an Overall Score (0-100) and 2-3 sentences of summary feedback.\n",
        "    \"\"\"\n",
        "\n",
        "    load_dotenv()\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert AI interviewer and evaluator for data science roles.\"},\n",
        "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        start = time.time()\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        evaluation = response.choices[0].message.content\n",
        "        print(f\"\\nAnswer Evaluation:\\n{evaluation}\")\n",
        "        print(f\"Took {round(end - start, 2)} seconds\\n\")\n",
        "\n",
        "        return evaluation\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating answer: {str(e)}\")\n",
        "        return \"Could not evaluate the answer due to an error.\"\n",
        "\n",
        "#----------------------------------------\n",
        "# INTERVIEW SESSION CLASS\n",
        "#----------------------------------------\n",
        "\n",
        "class InterviewSession:\n",
        "    def __init__(self, resume_path, job_description_path):\n",
        "        # Parse resume and job description\n",
        "        self.resume_text = read_file(resume_path)\n",
        "        self.job_description_text = read_file(job_description_path)\n",
        "\n",
        "        # Extract structured information\n",
        "        self.structured_resume = extract_resume_structure(resume_path)\n",
        "        self.structured_jd = extract_job_description_structure(self.job_description_text)\n",
        "\n",
        "        # Create personal profile from structured information\n",
        "        self.personal_profile = self.create_personal_profile()\n",
        "\n",
        "        # Initialize interview state\n",
        "        self.interview_dict = {}\n",
        "        self.question_num = 0\n",
        "        self.messages = []\n",
        "        self.current_question = None\n",
        "        self.follow_up_questions = []\n",
        "        self.audio_features = []\n",
        "        self.text_features = []\n",
        "\n",
        "        # Initialize RAG if available\n",
        "        try:\n",
        "            self.vector_db = initialize_rag(\"data_science.txt\")  # Use a local file path instead of GCS\n",
        "            self.retrieval_qa = setup_retrieval_qa(self.vector_db)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: RAG initialization failed: {str(e)}\")\n",
        "            self.vector_db = None\n",
        "            self.retrieval_qa = None\n",
        "\n",
        "    def create_personal_profile(self):\n",
        "        \"\"\"Create a personal profile from structured resume and job description\"\"\"\n",
        "        profile = {\n",
        "            'name': self.structured_resume.get('contact_info', {}).get('name', 'Candidate'),\n",
        "            'resume': self.resume_text,\n",
        "            'job_description': self.job_description_text,\n",
        "            'role': self.structured_jd.get('title', 'Data Scientist'),\n",
        "            'company': self.structured_jd.get('company', 'Company'),\n",
        "            'industry': self.structured_jd.get('industry', 'Technology'),\n",
        "            'experience': self.structured_resume.get('years_of_experience', '3-5'),\n",
        "            'education': [edu.get('degree', '') + ' from ' + edu.get('institution', '')\n",
        "                         for edu in self.structured_resume.get('education', [])],\n",
        "            'skills': {\n",
        "                'technical': self.structured_resume.get('skills', {}).get('technical', []),\n",
        "                'soft': self.structured_resume.get('skills', {}).get('soft', [])\n",
        "            },\n",
        "            'domain_expertise': self.structured_resume.get('domain_expertise', []),\n",
        "            'job_requirements': self.structured_jd.get('requirements', {}).get('required_skills', []),\n",
        "            'job_responsibilities': self.structured_jd.get('responsibilities', []),\n",
        "            'key_technologies': self.structured_jd.get('key_technologies', []),\n",
        "            'seniority_level': self.structured_jd.get('seniority_level', 'Mid-level')\n",
        "        }\n",
        "\n",
        "        # Create a skill match score\n",
        "        required_skills = set([skill.lower() for skill in profile['job_requirements']])\n",
        "        candidate_skills = set([skill.lower() for skill in profile['skills']['technical']])\n",
        "\n",
        "        if required_skills:\n",
        "            profile['skill_match_score'] = len(required_skills.intersection(candidate_skills)) / len(required_skills)\n",
        "        else:\n",
        "            profile['skill_match_score'] = 0\n",
        "\n",
        "        return profile\n",
        "\n",
        "    def start_interview(self):\n",
        "        \"\"\"Start the interview with the initial question\"\"\"\n",
        "        initial_question, self.messages = generate_initial_question(self.structured_resume, self.structured_jd)\n",
        "        self.current_question = initial_question\n",
        "        self.add_question(initial_question, 0)\n",
        "\n",
        "        # Generate follow-up questions\n",
        "        self.follow_up_questions = generate_follow_up_questions(self.messages.copy())\n",
        "\n",
        "        return initial_question\n",
        "\n",
        "    def add_question(self, question, question_num):\n",
        "        \"\"\"Add a question to the interview dictionary\"\"\"\n",
        "        self.interview_dict[question_num] = {\n",
        "            \"question\": question,\n",
        "            \"answer\": None,\n",
        "            \"evaluation\": None,\n",
        "            \"audio_features\": None,\n",
        "            \"text_features\": None\n",
        "        }\n",
        "        self.question_num = question_num\n",
        "\n",
        "    def add_answer(self, answer, audio_data=None):\n",
        "        \"\"\"Add an answer to the current question\"\"\"\n",
        "        question_num = self.question_num\n",
        "\n",
        "        # Process audio if available\n",
        "        audio_features = None\n",
        "        text_features = None\n",
        "        transcript = answer\n",
        "\n",
        "        if audio_data:\n",
        "            audio_result = process_audio_recording(audio_data)\n",
        "            if audio_result:\n",
        "                transcript = audio_result['transcript'] or answer\n",
        "                audio_features = audio_result['audio_features']\n",
        "\n",
        "                # Extract text features if we have audio length\n",
        "                if audio_features and 'audio_length' in audio_features:\n",
        "                    text_extractor = TextFeatureExtractor()\n",
        "                    text_features = text_extractor.extract_features(transcript, audio_features['audio_length'])\n",
        "\n",
        "        # Store the answer and features\n",
        "        self.interview_dict[question_num][\"answer\"] = transcript\n",
        "        self.interview_dict[question_num][\"audio_features\"] = audio_features\n",
        "        self.interview_dict[question_num][\"text_features\"] = text_features\n",
        "\n",
        "        # Generate expert answer for comparison\n",
        "        expert_answer = generate_expert_answer(\n",
        "            self.interview_dict[question_num][\"question\"],\n",
        "            self.retrieval_qa,\n",
        "            self.personal_profile\n",
        "        )\n",
        "\n",
        "        # Evaluate the answer\n",
        "        evaluation = evaluate_answer(\n",
        "            transcript,\n",
        "            expert_answer,\n",
        "            self.interview_dict[question_num][\"question\"],\n",
        "            audio_features,\n",
        "            text_features\n",
        "        )\n",
        "\n",
        "        self.interview_dict[question_num][\"evaluation\"] = evaluation\n",
        "        self.interview_dict[question_num][\"expert_answer\"] = expert_answer\n",
        "\n",
        "        return expert_answer, evaluation\n",
        "\n",
        "    def next_question(self):\n",
        "        \"\"\"Generate the next question based on the previous answer\"\"\"\n",
        "        if self.question_num == 0 and len(self.follow_up_questions) > 0:\n",
        "            # Use the first pre-generated follow-up for the second question\n",
        "            next_question = self.follow_up_questions[0]\n",
        "            self.add_question(next_question, 1)\n",
        "            return next_question\n",
        "\n",
        "        # For subsequent questions, generate dynamically based on previous answers\n",
        "        last_answer = self.interview_dict[self.question_num][\"answer\"]\n",
        "        next_question = generate_dynamic_question(\n",
        "            self.messages.copy(),\n",
        "            last_answer,\n",
        "            self.retrieval_qa,\n",
        "            self.personal_profile\n",
        "        )\n",
        "\n",
        "        self.add_question(next_question, self.question_num + 1)\n",
        "        return next_question\n",
        "\n",
        "    def get_interview_summary(self):\n",
        "        \"\"\"Generate a summary of the interview with feedback\"\"\"\n",
        "        load_dotenv()\n",
        "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        summary_prompt = f\"\"\"\n",
        "        You are an expert interviewer for data science roles. Analyze the complete interview below and provide a comprehensive summary with feedback.\n",
        "\n",
        "        Interview for Data Science Role:\n",
        "\n",
        "        {self.format_interview_transcript()}\n",
        "\n",
        "        Please provide:\n",
        "        1. Overall Assessment: Give a 2-3 paragraph summary of the candidate's performance\n",
        "        2. Key Strengths: List 3-5 specific strengths demonstrated in the interview\n",
        "        3. Areas for Improvement: List 2-3 specific areas where the candidate could improve\n",
        "        4. Technical Competency: Rate the candidate's technical knowledge (1-10) with brief explanation\n",
        "        5. Communication Skills: Rate the candidate's communication skills (1-10) with brief explanation\n",
        "        6. Recommendation: Provide a clear hiring recommendation (Strongly Recommend, Recommend, Consider, or Do Not Recommend)\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert AI interviewer and evaluator for data science roles.\"},\n",
        "            {\"role\": \"user\", \"content\": summary_prompt}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            start = time.time()\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=messages\n",
        "            )\n",
        "            end = time.time()\n",
        "\n",
        "            interview_summary = response.choices[0].message.content\n",
        "            print(f\"\\nInterview Summary:\\n{interview_summary}\")\n",
        "            print(f\"Took {round(end - start, 2)} seconds\\n\")\n",
        "\n",
        "            return interview_summary\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating interview summary: {str(e)}\")\n",
        "            return \"Could not generate interview summary due to an error.\"\n",
        "\n",
        "    def format_interview_transcript(self):\n",
        "        \"\"\"Format the interview transcript for the summary\"\"\"\n",
        "        transcript = \"\"\n",
        "        for q_num in sorted(self.interview_dict.keys()):\n",
        "            question = self.interview_dict[q_num][\"question\"]\n",
        "            answer = self.interview_dict[q_num][\"answer\"]\n",
        "            evaluation = self.interview_dict[q_num][\"evaluation\"]\n",
        "\n",
        "            transcript += f\"Q{q_num+1}: {question}\\n\\n\"\n",
        "            transcript += f\"A{q_num+1}: {answer}\\n\\n\"\n",
        "            transcript += f\"Evaluation: {evaluation}\\n\\n\"\n",
        "            transcript += \"---\\n\\n\"\n",
        "\n",
        "        return transcript\n",
        "\n",
        "#----------------------------------------\n",
        "# MAIN EXECUTION\n",
        "#----------------------------------------\n",
        "\n",
        "def save_interview_results(interview, alignment_analysis):\n",
        "    \"\"\"Save interview results to file\"\"\"\n",
        "    try:\n",
        "        # Create a timestamped filename\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        filename = f\"interview_results_{timestamp}.json\"\n",
        "\n",
        "        # Format results\n",
        "        results = {\n",
        "            \"candidate_name\": interview.personal_profile.get('name', 'Candidate'),\n",
        "            \"role\": interview.personal_profile.get('role', 'Data Scientist'),\n",
        "            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"resume_job_alignment\": alignment_analysis,\n",
        "            \"interview_transcript\": {},\n",
        "            \"overall_summary\": interview.get_interview_summary()\n",
        "        }\n",
        "\n",
        "        # Add interview transcript\n",
        "        for q_num in sorted(interview.interview_dict.keys()):\n",
        "            results[\"interview_transcript\"][f\"q{q_num+1}\"] = {\n",
        "                \"question\": interview.interview_dict[q_num][\"question\"],\n",
        "                \"answer\": interview.interview_dict[q_num][\"answer\"],\n",
        "                \"expert_answer\": interview.interview_dict[q_num].get(\"expert_answer\", \"\"),\n",
        "                \"evaluation\": interview.interview_dict[q_num].get(\"evaluation\", \"\"),\n",
        "                \"audio_features\": interview.interview_dict[q_num].get(\"audio_features\", {}),\n",
        "                \"text_features\": interview.interview_dict[q_num].get(\"text_features\", {})\n",
        "            }\n",
        "\n",
        "        # Save to file\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        print(f\"\\nInterview results saved to {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving interview results: {str(e)}\")\n",
        "\n",
        "def run_interview():\n",
        "    \"\"\"Main function to run the interview process\"\"\"\n",
        "    # Set file paths\n",
        "    resume_path = \"resume.pdf\"\n",
        "    jd_path = \"jd.txt\"\n",
        "\n",
        "    # Create interview session with enhanced resume and job description parsing\n",
        "    interview = InterviewSession(resume_path, jd_path)\n",
        "\n",
        "    # Analyze resume-job alignment\n",
        "    alignment_analysis = analyze_resume_job_alignment(interview.structured_resume, interview.structured_jd)\n",
        "    print(\"\\n===== RESUME-JOB ALIGNMENT ANALYSIS =====\")\n",
        "    print(f\"Overall Match Score: {alignment_analysis['overall_match_score']}/100\")\n",
        "    print(\"\\nStrengths:\")\n",
        "    for strength in alignment_analysis['strengths'][:3]:\n",
        "        print(f\"- {strength}\")\n",
        "    print(\"\\nAreas of Concern:\")\n",
        "    for concern in alignment_analysis['areas_of_concern'][:3]:\n",
        "        print(f\"- {concern}\")\n",
        "    print(\"\\nSuggested Interview Focus Areas:\")\n",
        "    for area in alignment_analysis['suggested_interview_focus_areas'][:3]:\n",
        "        print(f\"- {area}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Generate tailored questions based on profile\n",
        "    tailored_questions = generate_tailored_questions(interview.personal_profile)\n",
        "\n",
        "    # Use tailored questions if available\n",
        "    if tailored_questions and len(tailored_questions) >= 5:\n",
        "        # Add technical knowledge question as the initial question\n",
        "        interview.add_question(tailored_questions[0], 0)\n",
        "        interview.current_question = tailored_questions[0]\n",
        "        # Store the rest as follow-up questions\n",
        "        interview.follow_up_questions = tailored_questions[1:]\n",
        "    else:\n",
        "        # Fall back to standard question generation\n",
        "        initial_question = interview.start_interview()\n",
        "\n",
        "    print(\"\\n===== INTERVIEW STARTED =====\\n\")\n",
        "    print(f\"Interviewer: {interview.current_question}\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input (text or audio)\n",
        "        user_input = input(\"Your Answer (or type 'exit' to end the interview): \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # Process user's answer (assumes text input only)\n",
        "        expert_answer, evaluation = interview.add_answer(user_input)\n",
        "\n",
        "        # Generate next question\n",
        "        next_question = interview.next_question()\n",
        "\n",
        "        print(\"\\n----- Evaluation -----\")\n",
        "        print(evaluation)\n",
        "        print(\"\\n----- Expert Example Answer -----\")\n",
        "        print(expert_answer)\n",
        "        print(\"\\n----- Next Question -----\")\n",
        "        print(f\"Interviewer: {next_question}\\n\")\n",
        "\n",
        "    # Get interview summary\n",
        "    print(\"\\n===== INTERVIEW COMPLETED =====\\n\")\n",
        "    summary = interview.get_interview_summary()\n",
        "    print(\"\\n===== INTERVIEW SUMMARY =====\\n\")\n",
        "    print(summary)\n",
        "\n",
        "    # Save interview transcript and feedback\n",
        "    save_interview_results(interview, alignment_analysis)\n",
        "\n",
        "    return interview\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interview_session = run_interview()"
      ]
    }
  ]
}